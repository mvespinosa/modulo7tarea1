{"cells":[{"cell_type":"code","source":["from pyspark.sql import SQLContext, HiveContext\nsqlc = SQLContext(sc)\nhivec = HiveContext(sc)\nprint type(sqlc)\nprint type (hivec)"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["# Creacion de un dataframe de manera sintetica\niris_data = [(4.3,3.0,1.1,0.1,'Iris-setosa'), (7.0,3.2,4.7,1.4,'Iris-versicolor'), (6.9,3.2,5.7,2.3,'Iris-virginica')]\nprint iris_data\ndf = sqlContext.createDataFrame(iris_data,['sepal_length','sepal_width','petal_length','petal_width','specie'])\ndf.show()\ndf.printSchema()\ndf.collect()"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# Creacion de un dataframe de Spark a partir de un dataframe de Pandas\nspark_df = sqlContext.createDataFrame(pandas_df)\n"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Crear un dataframe a partir de un RDD usando reflection\nfrom pyspark.sql import Row\n# Cargamos el fichero de texto con el dataset de iris y lo convertimos en un RDD\nlineasRDD = sc.textFile('/FileStore/tables/vvlm20gs1474462720233/iris.csv')\n# Convertimos cada línea en una fila del futuro dataframe\nelementosRDD = lineasRDD.map(lambda l: l.split(\",\"))\n# Metemos cada uno de los campos separados por coma en un objeto tipo fila (Row) y le damos un nombre\nirisRDD = elementosRDD.map(lambda p: Row(sepal_length=float(p[0]), sepal_width=float(p[1]), petal_length=float(p[2]), \\\n          petal_width=float(p[3]), specie=p[4]))\n# Creamos el Dataframe a partir del RDD\nirisDF = sqlContext.createDataFrame(irisRDD)\nirisDF.show(3)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Crea un dataframe a partir de un RDD de manera programática\n# Importamos los datatypes\nfrom pyspark.sql.types import *\n\n# Cargamos el rdd desde un archivo de texto\nlineasRDD = sc.textFile('/FileStore/tables/vvlm20gs1474462720233/iris.csv')\n# Convertimos cada línea en una fila del futuro dataframe\nelementosRDD = lineasRDD.map(lambda l: l.split(\",\"))\nirisRDD = elementosRDD.map(lambda p: (float(p[0]), float(p[1]), float(p[2]), float(p[3]), p[4]))\n# Definimos el nombre y tipo de campos que queremos para nuestro dataframe\nfields = [StructField(\"sepal_length\", FloatType(), True), StructField(\"sepal_width\", FloatType(), True), \\\n          StructField(\"petal_length\", FloatType(), True), StructField(\"petal_width\", FloatType(), True), \\\n          StructField(\"specie\", StringType(), True)]\n#Definimos el esquema con los tipos de campos creados\nschema = StructType(fields)\n\n# Creamos el Dataframe a partir del RDD aplicando el esquema con los campos\nirisDF = sqlContext.createDataFrame(irisRDD, schema)\nirisDF.show(3)\n"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# Ver la estructura de un DataFrame\nirisDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# Creación de un RDD a partir de un DataFrame\nirisRDD = irisDF.select(\"sepal_length\", \"sepal_width\",\"specie\").rdd\nprint type(irisRDD)\nirisRDD.take(3)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# Creación de un dataset a partir de un fichero json\n#{\"name\":\"Michael\"}\n#{\"name\":\"Andy\", \"age\":30}\n#{\"name\":\"Justin\", \"age\":19}\n\n# Importamos los datatypes\nfrom pyspark.sql.types import *\njsonDF = sqlContext.read.format('json').load('/FileStore/tables/h9cow6461473884794846/people.json')\njsonDF.show()\n\n# Tambien podemos forzar el tipo de datos en el esquema antes de leer el fichero\n# Definicion del esquema\npeopleSchema = StructType([StructField(\"name\", StringType(), True), StructField(\"age\", IntegerType(), True)])\njsonStructDF = sqlContext.read.format('json').load('/FileStore/tables/h9cow6461473884794846/people.json', schema=peopleSchema)\njsonStructDF.show()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["\n# Creación de un dataset a partir de un fichero csv usando spark-csv\nfrom pyspark.sql.types import *\n\n# Utilizando cabecera e infiriendo el tipo de los campos, schema\nirisDfCsv = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferSchema='true') \\\n.load('/FileStore/tables/howzjb5q1474385450651/iris_with_header-f677d.csv')\nirisDfCsv.show(3)\n# Utilizando cabecera y definiendo el schema apriori\n# Definicion del esquema\nirisSchema = StructType([ \\\n    StructField(\"sepal_length\", FloatType(), True), \\\n    StructField(\"sepal_width\", FloatType(), True), \\\n    StructField(\"petal_length\", FloatType(), True), \\\n    StructField(\"petal_width\", FloatType(), True), \\\n    StructField(\"specie\", StringType(), True)])\nirisDfCsvStruct = sqlContext.read.format('com.databricks.spark.csv').options(header='true') \\\n.load('/FileStore/tables/howzjb5q1474385450651/iris_with_header-f677d.csv', schema = irisSchema)\n\n# display(irisDfCsvStruct)\n"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# Correlacion entre columnas de un DataFrame\nfrom pyspark.sql.functions import count\nfrom pyspark.sql.functions import corr\n\nprint \"Correlacion sepal_petal_length: {0}\".format(irisDfCsv.corr(\"sepal_length\", \"petal_length\"))\n# irisDfCsv.agg(corr(\"sepal_length\", \"petal_length\")).show()\nirisDfCsv.select(corr(\"sepal_length\", \"petal_length\").alias(\"corr_sepal_petal_length\"), \\\n                corr(\"sepal_width\", \"petal_width\").alias(\"corr_sepal_petal_width\")).show()\n\n\n"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# Reemplazo de ceros por otro valor\niris_dat = [(4.3,3.0, 0.0 ,0.1,'Iris-setosa'), (7.0,3.2,4.7,1.4,'Iris-versicolor'), (6.9,3.2,5.7, 0.0 ,'Iris-virginica')]\nirisZero = sqlContext.createDataFrame(iris_dat,['sepal_length','sepal_width','petal_length','petal_width','specie'])\nirisZero.show()\nirisZero.na.replace(0.0, 1.0,[\"petal_length\"]).show()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# Operaciones con join\niris_dat = [(4.3,3.0, 0.0 ,0.1,'Iris-setosa'), (7.0,3.2,4.7,1.4,'Iris-versicolor'), (6.9,3.2,5.7, 0.0 ,'Iris-virginica')]\niris_col = [('Iris-setosa', 'Naranja'), ('Iris-virginica', 'Violeta')]\n# Creamos un DataFrame basado en iris\nirisDf = sqlContext.createDataFrame(iris_dat,['sepal_length','sepal_width','petal_length','petal_width','specie'])\n# Creamos otro Dataframe con el color de cada especie, excepto para versicolor\nirisColDf = sqlContext.createDataFrame(iris_col, ['especie','color'])\n#irisDf.show()\nirisColDf.show()\n# Hacemos un inner join\nirisDf.join(irisColDf, irisDf.specie == irisColDf.especie, 'inner'). \\\nselect('sepal_length', 'sepal_width','petal_length','petal_width','specie','color'). \\\nshow()\n# Hacemos un left outer join\nirisDf.join(irisColDf, irisDf.specie == irisColDf.especie, 'left_outer'). \\\nselect('sepal_length', 'sepal_width','petal_length','petal_width','specie','color'). \\\nshow()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# Consultar tablas en HIVE\nsqlContext.tables(dbName='test').show()\nsqlContext.sql(\"USE default\")\nsqlContext.sql(\"SHOW TABLES\").show()\nirisFromHive = sqlContext.table(\"test.iris2\")\nirisFromHive.show(3)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# Interaccion con HIVE\n# Creación de una tabla con comandos HIVE con Spark SQL\n# Primero creamos la BBDD donde vamos a colocar la tabla\nsqlContext.sql(\"CREATE DATABASE IF NOT EXISTS test\")\nsqlContext.sql(\"SHOW DATABASES\").show()\n# Creamos la tabla HIVE\nsqlContext.sql(\"CREATE TABLE IF NOT EXISTS test.iris (sepal_length FLOAT, \\\nsepal_width FLOAT, petal_length FLOAT, petal_with FLOAT, specie STRING) \\\nROW FORMAT DELIMITED \\\nFIELDS TERMINATED BY ',' \\\nSTORED AS TEXTFILE\")\n# Vemos si la tabla se ha creado\nsqlContext.sql(\"USE test\")\nsqlContext.sql(\"SHOW TABLES\").show()\n# Cargamos los datos en la tabla desde un fichero\nsqlContext.sql(\"LOAD DATA INPATH '/FileStore/tables/ofv0n3w01474198366371/iris.csv' INTO TABLE test.iris\")\nsqlContext.sql(\"SELECT * FROM test.iris limit 5\").show()\n\n# Creación de un DataFrame a partir de una tabla HIVE\nirisFromHive = sqlContext.table(\"test.iris2\")\n\n# Creación de una tabla HIVE a partir de un dataframe existente\nirisDfCsv.write.saveAsTable('test.iris2')\nsqlContext.sql(\"SELECT * FROM test.iris2 limit 5\").show()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# Transformaciones: Actuando sobre las columnas de un dataframe\n# Crear un dataframes solo con la columna specie\nirisSpecie = irisDF.select('specie')\nirisSpecie.show(2)\n# Crear un dataframe con las columnas petal_length y especie\nirisSpecie2 = irisDF.select(irisDF.petal_length,irisDF.specie)\nirisSpecie2.show(2)\n# Crear un dataframe en el que se sume uno al petal_length y se le cambie el nombre\nirisSpecie3 = irisDF.select((irisDF.petal_length + 1.0).alias('petal_length+1'),irisDF.specie)\nirisSpecie3.show(2)\n"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# Crear un nuevo dataframe borrando una columna\nirisSinSpecie = irisDF.drop(irisDF.specie)\nirisSinSpecie.show(2)\n"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# Selección de filas con filter\n# Vemos el número de observaciones con la función count()\nprint(\"Numero de filas antes del filtrado \"+str(irisDF.count()))\n# Nos quedamos con las observaciones con sepal_length mayor que 5\nirisFiltrado = irisDF.filter(irisDF.sepal_length > 5.0)\nprint(\"Numero de filas despues del filtrado \"+str(irisFiltrado.count()))"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# Selección de los registros distintos\n# Vemos el número de observaciones con la función count()\nprint(\"Numero de filas totales \"+str(irisDF.count()))\n# Obtenemos las filas distintas\nprint(\"Numero de filas distintas \"+str(irisDF.distinct().count()))\n"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# Ordenacion de los dataframe\nirisDF.sort(irisDF.specie, ascending=False).show(2)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["# Expansion de un campo tipo array en varias filas\nfrom pyspark.sql import Row\nfrom pyspark.sql.functions import explode\ndata=[Row(specie=\"Virginica\",sepal_length_list=[0.1,2.3,3.1,4.0])]\nIrisDF=sqlContext.createDataFrame(data)\nIrisDF.show()\nIrisExplodeDF = IrisDF.select(IrisDF.specie, explode(IrisDF.sepal_length_list).alias(\"sepal_length\"))\nIrisExplodeDF.show()\n"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["# Operaciones de Group by. Funcion agg\nfrom pyspark.sql.functions import avg\nirisDFGroup = irisDF.groupBy(irisDF.specie)\nirisDFGroupCount = irisDFGroup.agg({\"*\":\"count\"})\nirisDFGroupCount.show()\n# irisDFGroupAVG = irisDFGroup.agg({\"sepal_length\":\"avg\",\"sepal_width\":\"avg\"})\n# Obtenemos el mismo resultado con la nomenclatura tipo función:\nirisDFGroupAVG = irisDFGroup.agg(avg(\"sepal_length\"), avg(\"sepal_width\"))\nirisDFGroupAVG.show()"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# Operaciones de Group by. Funciones count y avg\nirisDFGroup = irisDF.groupBy(irisDF.specie)\nirisDFGroup.count().show()\nirisDFGroup.avg().show()\nirisDFGroup.avg(\"sepal_length\").show()\nirisDF.groupBy().avg().show()\n"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["# Funciones definidas por el usuario UDF\nfrom pyspark.sql.types import FloatType\n# Definimos la udf indicando la función a aplicar, en este caso sumar 1 al argumento que se le pasa,\n# También se le pasa el tipo del valor retornado\nmasUno = udf(lambda s: s + 1,FloatType())\n# Aplicamos la funcion creada masUno al campo sepal_length para sumarle uno\nirisDFMasUno= irisDF.select(irisDF.specie,masUno(irisDF.sepal_length).alias('sepal_length_mas_uno'))\nirisDFMasUno.show(3)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["# Acciones. Ver todos los datos de un dataframe con collect()\n# La salida es una lista python, se puede limitar el número de registros utilizando la notacion de array \nirisDF.collect()[1:5]"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["# Acciones. Ver los datos parciales de un dataframe con take()\nirisDF.take(5)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["# Acciones. Ver los datos parciales de un dataframe con show()\nirisDF.show(5)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["# Acciones. Contar el número de registros de un dataframe\nirisDF.count()"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["# Acciones. Obtener un resumen de los campos\nirisDF.describe().show()"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["# Operaciones SQL sobre Dataframes\n# Es necesario registrar una tabla temporal ligada al dataframe\nirisDF.registerTempTable(\"iris_table\")\nsqlContext.sql(\"SHOW TABLES\").show()\nsqlContext.sql(\"SELECT * FROM iris_table where sepal_length > 5 limit 3\").show()\nsqlContext.sql(\"DROP TABLE iris_table\")\nsqlContext.sql(\"SHOW TABLES\").show()\n"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["# Operaciones SQL group by\n# Registramos la tabla temporal ligada al dataframe\nirisDF.registerTempTable(\"irisTabla\")\nsqlContext.sql(\"SELECT SPECIE, COUNT(*) AS NUMSPECIES \\\nFROM IRISTABLA GROUP BY SPECIE ORDER BY SPECIE ASC\").show()\n"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["sqlContext.tables().show()\n# sqlContext.sql(\"Drop table iris_table\")"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["# Vectores densos y spare\nfrom pyspark.mllib.linalg import Vectors\n\nvdense = Vectors.dense([0, 1.0, 0, 5.5])\nprint vdense\n\nvsparse1 = Vectors.sparse(4, {1: 1.0, 3: 5.5})\nprint vsparse1\n\nvsparse2 = Vectors.sparse(4, [(1, 1.0), (3, 5.5)])\nprint vsparse2\n\nvsparse3 = Vectors.sparse(4, [1, 3], [1.0, 5.5])\nprint vsparse3"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["# Labeled point\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.mllib.regression import LabeledPoint\n# Creacion a partir de un dense vector\nl1 = LabeledPoint(1.0, [1.0, 0.0, 3.0])\nprint l1\n# Creacion a partir de un sparse vector\nl2 = LabeledPoint(1.0, Vectors.sparse(3, [0, 2], [1.0, 3.0]))\nprint l2"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["# Regresión lineal con ml - creacion sintética de datos\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.ml.regression import LinearRegression\n## 1     3.600      79\n## 2     1.800      54\n## 3     3.333      74\n## 4     2.283      62\n## 5     4.533      85\n## 6     2.883      55\n\n#ignore = ['id', 'label', 'binomial_label']\n#assembler = VectorAssembler(\n#    inputCols=[x for x in df.columns if x not in ignore],\n#    outputCol='features')\n\n#assembler.transform(df)\n\n#df = sqlContext.createDataFrame([(1.0, 1.0, Vectors.dense(1.0)),(0.0, 2.0, Vectors.sparse(1, [], []))], [\"label\", \"weight\", \"features\"])\ndf = sqlContext.createDataFrame([(3.6, Vectors.dense(79)),(1.8, Vectors.dense(54)),(3.333,Vectors.dense(74))], [\"duration\", \"features\"])\n\ndf.show()\n\nlr = LinearRegression(maxIter=5, regParam=0.0, solver=\"normal\", labelCol=\"duration\")\nprint type(lr)\n\nmodel = lr.fit(df)\nprint type(model)\n\nprint model.coefficients\nprint model.intercept\n#print model.params\n"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["# Regresión lineal con ml a partir de un DataFrame\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.evaluation import RegressionEvaluator\n# Dataset faithful\n#        eruptions  waiting\n## 1     3.600      79\n## 2     1.800      54\n## 3     3.333      74\n## 4     2.283      62\n## 5     4.533      85\n\n# Creamos el dataframe a partir de un fichero csv\nfaithfulDfCsv = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferSchema='true') \\\n.load('/FileStore/tables/k250v29z1473806605274/faithful.csv')\n\n# Creamos un VectorAssembler para tener todas las variables predictoras en un vector\nvecAssembler = VectorAssembler(inputCols=[\"waiting\"], outputCol=\"features\")\nvaDf = vecAssembler.transform(faithfulDfCsv)\n\nvaDf.cache()\nvaDf.show(3)\n\n# Creamos el conjunto de entrenamiento y test\nsplits = vaDf.randomSplit([0.7, 0.3], 1234)\nvaDfTraining = splits[0]\nvaDfTest = splits[1]\n\n# Definimos el algoritmo de regresión lineal, indicando la columna target=labelCol\nlr = LinearRegression(maxIter=5, regParam=0.0, solver=\"normal\", labelCol=\"eruptions\", featuresCol=\"features\")\nlr.setRegParam(0.0)\nlr.setElasticNetParam(0.0)\n\n# Obtenemos el modelo una vez aplicado el algoritmo al conjunto de entrenamiento\nLRmodel = lr.fit(vaDfTraining)\nprint type(LRmodel)\n\n# Mostramos los coeficientes y el intercept de la regresión lineal\nprint \"Intercept: {0}\".format(LRmodel.intercept)\nprint \"Coeficientes: {0}\".format(LRmodel.coefficients)\n\n# Obtenemos el dataframe con las predicciones sobre el conjunto de test \nprediccion_faithful = LRmodel.transform(vaDfTest)\nprediccion_faithful.cache()\nprediccion_faithful.show(5)\n\n# Aplicamos un evaluador para conocer el error en entrenamiento y test\nevaluator = RegressionEvaluator(labelCol=\"eruptions\")\nprint \"RMSE en test: {0}\".format(evaluator.evaluate(prediccion_faithful,{evaluator.metricName: \"rmse\"}))\nprint \"RMSE en training: {0}\".format( evaluator.evaluate(LRmodel.transform(vaDfTraining), {evaluator.metricName: \"rmse\"}))"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["# Regresión lineal con Validacion Cruzada\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.tuning import ParamGridBuilder\nfrom pyspark.ml.tuning import CrossValidator\nfrom pyspark.ml.evaluation import RegressionEvaluator\n# Dataset faithful\n#        eruptions  waiting\n## 1     3.600      79\n## 2     1.800      54\n## 3     3.333      74\n## 4     2.283      62\n## 5     4.533      85\n\n# Creamos el dataframe a partir de un fichero csv\nfaithfulDfCsv = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferSchema='true') \\\n.load('/FileStore/tables/k250v29z1473806605274/faithful.csv')\n\n# Creamos un VectorAssembler para tener todas las variables predictoras en un vector\nvecAssembler = VectorAssembler(inputCols=[\"waiting\"], outputCol=\"features\")\nvaDf = vecAssembler.transform(faithfulDfCsv)\n\nvaDf.cache()\nvaDf.show(3)\n\n# Creamos el conjunto de entrenamiento y test\nsplits = vaDf.randomSplit([0.7, 0.3], 1234)\nvaDfTraining = splits[0]\nvaDfTest = splits[1]\n\n# Creacion del Modelo de LR: Ajuste con k-fold y grid de parámetros \nlr = LinearRegression(labelCol=\"eruptions\")\nlrgrid = ParamGridBuilder().addGrid(lr.regParam, [0.0, 0.01, 0.05, 0.5]).addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]).build()\nlrevaluator = RegressionEvaluator(labelCol=\"eruptions\")\ncv = CrossValidator(estimator=lr, estimatorParamMaps=lrgrid, evaluator=lrevaluator, numFolds=5)\ncvModel = cv.fit(vaDf)\n\nprint type(cvModel)\nprint type(cvModel.bestModel)\n\n# Mostramos los coeficientes y el intercept de la regresión lineal\nprint \"Intercept: {0}\".format(cvModel.bestModel.intercept)\nprint \"Coeficientes: {0}\".format(cvModel.bestModel.coefficients)\n\n# Realizamos la prediccion sobre el conjunto de test\nprediccionTestCsv = cvModel.transform(vaDfTest)\n\n# Aplicamos el evaluador para conocer el error en el dataset de test\nprint \"RMSE en test: {0}\".format(lrevaluator.evaluate(prediccionTestCsv,{evaluator.metricName: \"rmse\"}))"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["# Regresión lineal con ml a partir de un DataFrame convencional\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.tuning import ParamGridBuilder\nfrom pyspark.ml.tuning import CrossValidator\nfrom pyspark.ml.evaluation import RegressionEvaluator\n# Dataset faithful\n## 1     3.600      79\n## 2     1.800      54\n## 3     3.333      74\n## 4     2.283      62\n## 5     4.533      85\n\n# Creamos el dataframe a partir de un fichero csv\nfaithfulDfCsv = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferSchema='true') \\\n.load('/FileStore/tables/k250v29z1473806605274/faithful.csv')\n\nfaithfulDfCsv.cache()\n\nfaithfulDfCsv.show(3)\n\nvecAssembler = VectorAssembler(inputCols=[\"waiting\"], outputCol=\"features\")\nvaDf = vecAssembler.transform(faithfulDfCsv)\n\nsplits = vaDf.randomSplit([0.7, 0.3], 1234)\nvaDfTraining = splits[0]\nvaDfTest = splits[1]\n\n# Ajuste tradicional training-test\nlr = LinearRegression(maxIter=5, regParam=0.0, solver=\"normal\", labelCol=\"eruptions\")\n#print type(lr)\n\nmodel = lr.fit(vaDfTraining)\nprint type(model)\n\nprint model.coefficients\nprint model.intercept\n\nprediccion_faithful = model.transform(vaDfTest)\nprediccion_faithful.cache()\n\nprediccion_faithful.show(5)\nevaluator = RegressionEvaluator(labelCol=\"eruptions\")\nprint evaluator.evaluate(prediccion_faithful,{evaluator.metricName: \"rmse\"})\nprint evaluator.evaluate(model.transform(vaDfTraining), {evaluator.metricName: \"rmse\"})\n\n# Ajuste con k-fold\nlr = LinearRegression(labelCol=\"eruptions\")\ngrid = ParamGridBuilder().addGrid(lr.maxIter, [1, 5]).addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]).build()\ncv = CrossValidator(estimator=lr, estimatorParamMaps=grid, evaluator=evaluator, numFolds=5)\nprint type(cv)\ncvModel = cv.fit(vaDf)\n\nprediccion_test = cvModel.transform(vaDfTest)\n# prediccion_test.show()\nprint evaluator.evaluate(prediccion_test, {evaluator.metricName: \"rmse\"})\n\nprint type(cvModel.bestModel)\nprint type(cvModel)\nprint cvModel.bestModel.coefficients\nprint cvModel.bestModel.intercept\n#print cvModel.extractParamMap()\n\n\n\n"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["# Regresión lineal con ml\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n## 1     3.600      79\n## 2     1.800      54\n## 3     3.333      74\n## 4     2.283      62\n## 5     4.533      85\n## 6     2.883      55\n\n# Creamos un DataFrame con algunos de los valores del dataset faithful\ndf = sqlContext.createDataFrame([(3.6,79),(1.8,54),(3.333,74),(2.283,62),(4.533,85)], [\"duration\", \"eruptions\"] )\ndf.show()\n\nsplits = df.randomSplit([0.7, 0.3], 24)\nprint(splits[0].count())\nprint(splits[1].count())\n#dataset = sqlContext.range(0, 100).select((col(\"id\") % 3).alias(\"key\"))\n#sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n#sampled.groupBy(\"key\").count().orderBy(\"key\").show()\n# sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n# Realizamos un assebler para unir todas las columnas predictoras del dataset en un vector de features\nvecAssembler = VectorAssembler(inputCols=[\"eruptions\"], outputCol=\"features\")\nva = vecAssembler.transform(df)\nva.show()\n# Definimos la función de linear regression a aplicar\nlr = LinearRegression(maxIter=5, regParam=0.0, solver=\"normal\", labelCol=\"duration\")\n# Construimos el modelo aplicándolo sobre el dataset que va a ser nuestro conjunto de test\nmodel = lr.fit(va)\n# Mostramos los valores de los coeficientes y el punto de paso de la recta por el eje y\nprint model.coefficients\nprint model.intercept\n\n# Predicción con regresión lineal sobre un conjunto de test y evaluación del modelo\nfrom pyspark.ml.evaluation import RegressionEvaluator\ntest0 = sqlContext.createDataFrame([(2.283,Vectors.dense(62)), (1.8,Vectors.dense(54))], [\"duration\", \"features\"])\ntest0.show()\nprediccion_test = model.transform(test0)\nprediccion_test.show()\nevaluator = RegressionEvaluator(labelCol=\"duration\")\nprint(evaluator.evaluate(prediccion_test, {evaluator.metricName: \"rmse\"}))\nprediccion_training = model.transform(va)\nprediccion_training.show()\nprint(evaluator.evaluate(prediccion_training, {evaluator.metricName: \"rmse\"}))"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["# Regresión lineal con Cross Validation\n\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.tuning import ParamGridBuilder\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.tuning import CrossValidator\n## 1     3.600      79\n## 2     1.800      54\n## 3     3.333      74\n## 4     2.283      62\n## 5     4.533      85\n## 6     2.883      55\n\n# Creamos un DataFrame con algunos de los valores del dataset faithful\ndataset = sqlContext.createDataFrame([(3.6,79),(1.8,54),(3.333,74),(2.283,62),(4.533,85)], [\"label\", \"eruptions\"] )\ndataset.show()\n\n# Realizamos un assebler para unir todas las columnas predictoras del dataset en un vector de features\nvecAssembler = VectorAssembler(inputCols=[\"eruptions\"], outputCol=\"features\")\nvacross = vecAssembler.transform(dataset)\nvacross.show()\n\nlr = LinearRegression()\ngrid = ParamGridBuilder().addGrid(lr.maxIter, [0, 1]).build()\nevaluator = RegressionEvaluator()\ncv = CrossValidator(estimator=lr, estimatorParamMaps=grid, evaluator=evaluator)\nprint type(cv)\ncvModel = cv.fit(vacross)\n\ntestcross = sqlContext.createDataFrame([(2.283,Vectors.dense(62)), (1.8,Vectors.dense(54))], [\"label\", \"features\"])\ntestcross.show()\nprediccion_test = cvModel.bestModel.transform(testcross)\nprediccion_test.show()\nevaluator.evaluate(cvModel.transform(testcross), {evaluator.metricName: \"rmse\"})\n\nprint type(cvModel.bestModel)\nprint type(cvModel)\nprint cvModel.bestModel.coefficients\nprint cvModel.bestModel.intercept\n\n"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["# Regresion logistica\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.sql.types import *\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import OneHotEncoder\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n# Nos hemos de asegurar que el target sea DoubleType\n# Definicion del esquema\nadmisionSchema = StructType([ \\\n    StructField(\"admit\", BooleanType(), True), \\\n    StructField(\"gre\", DoubleType(), True), \\\n    StructField(\"gpa\", DoubleType(), True), \\\n    StructField(\"rank\", StringType(), True)])\nadmisionDfCsv = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferSchema='true') \\\n.load('/FileStore/tables/z1wr0h6j1473892626476/election.csv', schema=eleccionSchema)\n\nadmisionDfCsv.printSchema()\n# Transformamos la variable rank de tipo String en una lista de valores numéricos\nindexer = StringIndexer(inputCol=\"rank\", outputCol=\"ranknum\")\nmodelInd = indexer.fit(admisionDfCsv)\nadmisionDfInd = modelInd.transform(admisionDfCsv)\n\n# Transformamos la columna con la variable categórica ranknum en variables dummies\nencoder = OneHotEncoder(inputCol=\"ranknum\", outputCol=\"dummiesRank\", dropLast=False)\nadmisionDfdumm = encoder.transform(eleccionDfInd)\n\n# Realizamos un assebler para unir todas las columnas predictoras en un vector de features\nvecAssembler = VectorAssembler(inputCols=[\"gre\",\"gpa\",\"dummiesRank\"], outputCol=\"features\")\nadmisionAssemblerDf = vecAssembler.transform(admisionDfdumm)\n# admisionAssemblerDf.show(3)\n\n# Obtenemos los DataFrames de entrenamiento y test\nsplitado = admisionAssemblerDf.randomSplit([0.7, 0.3], 124)\nadmisionTrainingDf = splitado[0]\nadmisionTestDf = splitado[1]\n\n# Definimos el modelo de Regresion Logistica y lo aplicamos al DataFrame de training\nlr = LogisticRegression(maxIter=5, regParam=0.0, labelCol=\"admit\")\nLrModel = lr.fit(admisionTrainingDf)\n# Mostramos los coeficientes y el intercept de la regresión lineal\nprint \"Intercept: {0}\".format(LrModel.intercept)\nprint \"Coeficientes: {0}\".format(LrModel.coefficients)\n\n# Aplicamos el modelo al conjunto de Test\nprediccionTestDf = LrModel.transform(admisionTestDf)\nprediccionTestDf.show(3)\n\n# Evaluamos el resultado mostrando el area bajo la  ROC\nevaluator = BinaryClassificationEvaluator(labelCol=\"admit\")\nprint \"Area bajo la curva ROC: {0}\".format(evaluator.evaluate(prediccionTestDf, \\\n                                                    {evaluator.metricName: \"areaUnderROC\"}))\n"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["# Regresion logistica con variables dummy\nencoder = OneHotEncoder(inputCol=\"indexed\", outputCol=\"features\")"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["# Clasificador multiclase con RandomForest\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nirisDfCsv = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferSchema='true') \\\n.load('/FileStore/tables/fjobo1ml1473803578241/iris_with_header-f677d.csv')\n\n# Transformamos la etiqueta (specie) de String a Double en varios niveles\nindexer = StringIndexer(inputCol=\"specie\", outputCol=\"specieNum\")\nmodelInd = indexer.fit(irisDfCsv)\nirisDfInd = modelInd.transform(irisDfCsv)\n\n# Realizamos un assebler para unir todas las columnas predictoras del dataset en un vector de features\nvecAssembler = VectorAssembler(inputCols=[\"sepal_length\",\"sepal_width\",\"petal_length\", \"petal_width\"], \\\n                               outputCol=\"features\")\nirisDfAssembled = vecAssembler.transform(irisDfInd)\n# irisDfAssembled.show(3)\n\n# Creamos los DataFrame de entrenamiento y test\nsplitado = irisDfAssembled.randomSplit([0.7, 0.3], 124)\nirisDfTraining = splitado[0]\nirisDfTest = splitado[1]\n\n# Definimos el algoritmo de RandomForest y creamos el modelo \nrf = RandomForestClassifier(numTrees=20, maxDepth=2, labelCol=\"specieNum\", seed=42)\nrfModel = rf.fit(irisDfTraining)\n\n# Aplicamos el modelo sobre el conjunto de test para obtener las prediciones \nprediccionDfTest = rfModel.transform(irisDfTest)\nprediccionDfTest.show(5)\n\n# Aplicamos el evaluador multiclase par ver la eficiencia del modelo\nevaluator = MulticlassClassificationEvaluator(labelCol=\"specieNum\")\nprint 'Precision: {0}, Recall: {1}, F1: {2}'.format(evaluator.evaluate(prediccionDfTest, {evaluator.metricName: \"precision\"}), \\\n                                           evaluator.evaluate(prediccionDfTest, {evaluator.metricName: \"recall\"}), \\\n                                           evaluator.evaluate(prediccionDfTest, {evaluator.metricName: \"f1\"}))"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["# Pipe: Clasificador multiclase con RandomForest\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nirisDfCsv = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferSchema='true') \\\n.load('/FileStore/tables/howzjb5q1474385450651/iris_with_header-f677d.csv')\n\n# Creamos los DataFrame de entrenamiento y test\nsplitado = irisDfCsv.randomSplit([0.7, 0.3], 124)\nirisDfTraining = splitado[0]\nirisDfTest = splitado[1]\n\n# Transformamos la etiqueta (specie) de String a Double en varios niveles\nindexer = StringIndexer(inputCol=\"specie\", outputCol=\"specieNum\")\n# Realizamos un assebler para unir todas las columnas predictoras del dataset en un vector de features\nvecAssembler = VectorAssembler(inputCols=[\"sepal_length\",\"sepal_width\",\"petal_length\", \"petal_width\"], \\\n                               outputCol=\"features\")\n# Definimos el algoritmo de RandomForest y creamos el modelo \nrf = RandomForestClassifier(numTrees=20, maxDepth=2, labelCol=\"specieNum\", seed=42)\n# Definimos el pipeline con las operaciones en orden\npipeline = Pipeline(stages=[indexer, vecAssembler, rf])\n\n# Creamos el modelo aplicando el pipeline al conjunto de entrenamiento\npipeModel = pipeline.fit(irisDfTraining)\n\n# Aplicamos el modelo sobre el conjunto de test para obtener las prediciones \nprediccionDfTest = pipeModel.transform(irisDfTest)\nprediccionDfTest.show(5)"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["# Clustering con K-means\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.clustering import KMeans\nirisDfCsv = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferSchema='true') \\\n.load('/FileStore/tables/howzjb5q1474385450651/iris_with_header-f677d.csv')\n\n# Realizamos un assebler para unir todas las columnas predictoras del dataset en un vector de features\nvecAssembler = VectorAssembler(inputCols=[\"sepal_length\",\"sepal_width\",\"petal_length\", \"petal_width\"], outputCol=\"features\")\nirisAssembled = vecAssembler.transform(irisDfCsv)\n\n# Aplicamos el algoritmo K-means con un número de centroides de 3\nkmeans = KMeans(k=3, featuresCol=\"features\", predictionCol=\"prediccion\", seed=1234)\nmodel = kmeans.fit(irisAssembled)\n\n# Mostramos los valores para los centroides\ncenters = model.clusterCenters()\nfor i in range(3):\n  print \"Centroide del cluster {0}: {1}\".format(i, centers[i])\n\n# Vemos el resultado del clustering\n# Los identificadores de los grupos (prediccion) es un entero comenzando por el 0\ntransformed = model.transform(irisAssembled)\ntransformed.show(5)\n\n# Vemos el número de observaciones para cada cluster\nirisDFGroup = transformed.groupBy(transformed.prediccion)\nirisDFGroup.agg({\"*\":\"count\"}).show()\n"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["# Transformadores. VectorAssembler\nfrom pyspark.ml.feature import VectorAssembler\nirisDfCsv = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferSchema='true') \\\n.load('/FileStore/tables/howzjb5q1474385450651/iris_with_header-f677d.csv')\n\n# Mostramos el DataFrame original\nirisDfCsv.show(2)\n# Realizamos un assebler para unir todas las columnas predictoras del dataset en un vector de features\nvecAssembler = VectorAssembler(inputCols=[\"sepal_length\",\"sepal_width\",\"petal_length\", \"petal_width\"], \\\n                               outputCol=\"features\")\nirisAssembled = vecAssembler.transform(irisDfCsv)\nirisAssembled.show(2)\n"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["# Assemblers\nfrom pyspark.ml.feature import VectorAssembler\nignore = ['specie']\nassembler = VectorAssembler(\n    inputCols=['petal_length', 'petal_width', 'sepal_length', 'sepal_width'],\n    outputCol='features')\n\ndv = assembler.transform(irisDF)\ndv.select(dv.features).show(3)\nprint type(dv.features)\n\ndf = sqlContext.createDataFrame([(1, 0, 3)], [\"a\", \"b\", \"c\"])\nvecAssembler = VectorAssembler(inputCols=[\"a\", \"b\", \"c\"], outputCol=\"features\")\n#vecAssembler.transform(df).head().features\nva = vecAssembler.transform(df)\nprint type(va.features)\n"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["# Ejemplo randomForest con libreria mllib\n\nfrom pyspark.mllib.regression import LabeledPoint\nfrom pyspark.mllib.tree import RandomForest\n\n# Creamos una función para pasar las etiquetas a numérico que aplicaremos en el map()\ndef getLabelNum(label):\n  if label == 'Iris-virginica':\n    return 0.0\n  elif label == 'Iris-setosa':\n    return 1.0\n  else:\n    return 2.0\n\nlineasRDD = sc.textFile('/FileStore/tables/vvlm20gs1474462720233/iris.csv')\n# Convertimos cada línea en una fila del futuro dataframe\nelementosRDD = lineasRDD.map(lambda l: l.split(\",\"))\n# Metemos cada uno de los campos separados por coma en un punto etiquetado\nirisRDD = elementosRDD.map(lambda p: LabeledPoint(getLabelNum(p[4]), [p[0],p[1],p[2],p[3]]))\nprint irisRDD.take(3)\n# Creamos el modelo randomForest\nmodel = RandomForest.trainClassifier(irisRDD, numClasses=3, categoricalFeaturesInfo={}, \\\n                                     numTrees=1, maxDepth = 2, seed=42)\nprint \"Numero de nodos de los arboles {0}\".format(model.totalNumNodes())\nprint(model.toDebugString())\n# Creamos un RDD sobre el que realizar predicciones\ntestRDD = sc.parallelize([[0.2,3.1,1.2,2.0], [1.4,2.8,3.0,4.0]])\nprint \"Prediccion: {0}\".format(model.predict(testRDD).collect())"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["from pyspark.sql import Row\nrow = Row(name=\"Alice\", age=11)\nprint type(row)\nprint row\nprint row['name']\nprint row.age\nprint type(row.name)\nprint type(row.age)\nprint \"Spark version \" + sc.version"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["data = [('Alice',11), ('Robert', 14)]\nprint data\ndf = sqlContext.createDataFrame(data,['name','age'])\ndf.take(2)\ndf.show()"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["#dbutils.fs.ls(\"file:/home\")\n#dbutils.fs.ls(\"dbfs:/databricks-datasets/samples\")\ndbutils.fs.ls(\"dbfs:/FileStore/tables\")\n#dbutils.fs.rm(\"dbfs:/FileStore/tables/hev5dygo1473887816783\")"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["dbutils.fs.ls(\"dbfs:/FileStore/tables/ofv0n3w01474198366371\")"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["dbutils.fs.ls(\"dbfs:/FileStore/tables/test\")"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["dbutils.fs.rm(\"dbfs:/FileStore/tables/fjobo1ml1473803578241/iris_with_header-f677d.csv\")"],"metadata":{},"outputs":[],"execution_count":53}],"metadata":{"name":"edx-dataframes","notebookId":3436988668648363},"nbformat":4,"nbformat_minor":0}
